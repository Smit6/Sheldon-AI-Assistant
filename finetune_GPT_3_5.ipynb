{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheldon AI Assistant\n",
    "#### Dataset: Sheldon Dialogues\n",
    "<ul>This dataset is based on a character of Sheldon Cooper from The Big Bang Theory television show.</ul><a>https://huggingface.co/datasets/fenilgandhi/sheldon_dialogues</a>\n",
    "\n",
    "#### Objective\n",
    "<ul>Create an AI assistant that chats like Sheldon Cooper from The Big Bang Theory, leveraging fine-tuning of GPT 3.5 Turbo.</ul>\n",
    "\n",
    "## Table of Contents\n",
    "<ol>\n",
    "<li><a href = '#dw'>Data Wrangling</a></li>\n",
    "<li><a href = '#pdft'>Preparing Data For Fine-tuning</a></li>\n",
    "<li><a href = '#fec'>Format Error Checks</a></li>\n",
    "<li><a href = '#ct'>Count Tokens</a></li>\n",
    "<li><a href = '#pft'>Pricing For Fine-tuning</a></li>\n",
    "<li><a href = '#dft'>Upload Data For Fine-tuning</a></li>\n",
    "<li><a href = '#ftj'>Create A Fine-Tuning Job</a></li>\n",
    "<li><a href = '#ftvgpt'>Fine-tuned Model vs. GPT 3.5 Turbo</a></li>\n",
    "<li><a href = '#c'>Conclusion</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'dw'></a>\n",
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('sheldon.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] <<SYS>>\\n    Assume you are a theoretical physicist  by the name Sheldon living in USA. You have a strict adherence\\n    to routine and hygiene, an overly intellectual personality, a tenuous understanding of irony, sarcasm\\n    and humor, and a general lack of humility or empathy.\\n\\n    If a question does not make any sense, or is not factually coherent, you reply wittly with a sarcasm or\\n    outright denial with reasoning instead of answering something not correct. If you don't know the answer\\n    to a question, please don't share false information.\\n    <</SYS>>\\n\\n    Mmm, gentlemen, I put it to you, the worst tapioca pudding is better than the best pudding of any other flavour.\\n    [/INST]\\n    First off, that is axiomatically wrong, because the best pudding is chocolate. Secondly, the organic structure of tapioca makes it a jiggling bowl of potential death. It is extracted from the plant…\\n    \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data point\n",
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Sheldon assistant's characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Assume you are a theoretical physicist by the name Sheldon living in USA. You have a strict adherence to routine and hygiene, an overly intellectual personality, a tenuous understanding of irony, sarcasm and humor, and a general lack of humility or empathy. If a question does not make any sense, or is not factually coherent, you reply wittly with a sarcasm or outright denial with reasoning instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheldon_characteristics = df['text'].iloc[0].split('<</SYS>>')[0]\n",
    "# Remove the initial <s>[INST] <<SYS>>\\n\n",
    "sheldon_characteristics = sheldon_characteristics.replace('<s>[INST] <<SYS>>\\n', '')\n",
    "# Remove newlines\n",
    "sheldon_characteristics = sheldon_characteristics.replace('\\n', ' ')\n",
    "# Remove whitespaces\n",
    "sheldon_characteristics = ' '.join(sheldon_characteristics.split())\n",
    "sheldon_characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mmm, gentlemen, I put it to you, the worst tapioca pudding is better than the best pudding of any other flavour.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = df['text'].iloc[0].split('<</SYS>>')[1].split('[/INST]')\n",
    "conv[0].replace('\\n', '')\n",
    "# Replace white spaces\n",
    "conv[0] = ' '.join(conv[0].split())\n",
    "conv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = df['text'].iloc[0].split('<</SYS>>')[1].split('[/INST]')\n",
    "conv[0].replace('\\n', '')\n",
    "# Replace white spaces\n",
    "conv[0] = ' '.join(conv[0].split())\n",
    "\n",
    "# Apply above logic to all rows\n",
    "df['user'] = df['text'].apply(lambda x: x.split('<</SYS>>')[1].split('[/INST]')[0].replace('\\n', '')).apply(lambda x: ' '.join(x.split()))\n",
    "df['system'] = df['text'].apply(lambda x: x.split('<</SYS>>')[1].split('[/INST]')[1].replace('\\n', '')).apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>system</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mmm, gentlemen, I put it to you, the worst tap...</td>\n",
       "      <td>First off, that is axiomatically wrong, becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ah, no kidding! A Fu Man Chu? A handlebar pencil?</td>\n",
       "      <td>It is extracted from the plant…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alright Sheldon, why is tapioca…</td>\n",
       "      <td>Tapioca is extracted from the root of the plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feel better now?</td>\n",
       "      <td>It is also indigenous to Brazil, as is the Coc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fo’ shizzle.</td>\n",
       "      <td>Hey it’s true, Kripke lacks the basic social s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                user  \\\n",
       "0  Mmm, gentlemen, I put it to you, the worst tap...   \n",
       "1  Ah, no kidding! A Fu Man Chu? A handlebar pencil?   \n",
       "2                   Alright Sheldon, why is tapioca…   \n",
       "3                                   Feel better now?   \n",
       "4                                       Fo’ shizzle.   \n",
       "\n",
       "                                              system  \n",
       "0  First off, that is axiomatically wrong, becaus...  \n",
       "1                    It is extracted from the plant…  \n",
       "2  Tapioca is extracted from the root of the plan...  \n",
       "3  It is also indigenous to Brazil, as is the Coc...  \n",
       "4  Hey it’s true, Kripke lacks the basic social s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['user', 'system']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'pdft'></a>\n",
    "### Prepare dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "# Preaper data in json format for tuning\n",
    "for i in range(df.shape[0]):\n",
    "    dataset.append({\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': sheldon_characteristics\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': df['user'].iloc[i]\n",
    "            },\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': df['system'].iloc[i]\n",
    "            }\n",
    "            \n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': \"Assume you are a theoretical physicist by the name Sheldon living in USA. You have a strict adherence to routine and hygiene, an overly intellectual personality, a tenuous understanding of irony, sarcasm and humor, and a general lack of humility or empathy. If a question does not make any sense, or is not factually coherent, you reply wittly with a sarcasm or outright denial with reasoning instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
       "  {'role': 'user',\n",
       "   'content': 'Mmm, gentlemen, I put it to you, the worst tapioca pudding is better than the best pudding of any other flavour.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'First off, that is axiomatically wrong, because the best pudding is chocolate. Secondly, the organic structure of tapioca makes it a jiggling bowl of potential death. It is extracted from the plant…'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11217"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will finetune GPT 3.5 using 1000 examples only to reduce finetuning cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = dataset[1000:1100]\n",
    "dataset = dataset[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'fec'></a>\n",
    "### Format error checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ct'></a>\n",
    "### Count Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import numpy as np\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 125, 291\n",
      "mean / median: 153.422, 150.0\n",
      "p5 / p95: 132.0, 180.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 168\n",
      "mean / median: 19.873, 15.0\n",
      "p5 / p95: 5.0, 40.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'pft'></a>\n",
    "### Pricing for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~153422 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~460266 tokens\n",
      "See pricing page to estimate total costs\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "print(\"See pricing page to estimate total costs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_jsonl(conversations, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for conversation in conversations:\n",
    "            json_line = json.dumps(conversation)\n",
    "            file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "save_to_jsonl(dataset, 'sheldon_tasks_train.jsonl')\n",
    "\n",
    "# train dataset\n",
    "save_to_jsonl(valid_dataset, 'sheldon_tasks_validation.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'dft'></a>\n",
    "### Upload data for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "training_file_name = 'sheldon_tasks_train.jsonl'\n",
    "\n",
    "# train dataset\n",
    "validation_file_name = 'sheldon_tasks_validation.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"ADD YOUR API KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def upload_file_get_fileid(api_key, file_name):\n",
    "    headers = {\n",
    "        \"Authorization\": f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "    files = {\n",
    "        \"purpose\": (None, \"fine-tune\"),\n",
    "        \"file\": (file_name, open(file_name, \"rb\")),\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/files\", headers=headers, files=files)\n",
    "\n",
    "    response = response.json()\n",
    "    file_id = response[\"id\"]\n",
    "\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_id = upload_file_get_fileid(api_key, training_file_name)\n",
    "validation_file_id = upload_file_get_fileid(api_key, validation_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('file-BAnRxmSzDZZwqihruqCOHh5W', 'file-ZOpm45GLezNpuiBN0BwJee0n')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file_id, validation_file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ftj'></a>\n",
    "### Create a fine-tuning job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finetuning_job(api_key, model, suffix):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"training_file\": training_file_id,\n",
    "        \"validation_file\": validation_file_id,\n",
    "        \"model\": model,\n",
    "        \"suffix\": suffix\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/fine_tuning/jobs\", headers=headers, json=data)\n",
    "\n",
    "    response = response.json()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 'fine_tuning.job',\n",
       " 'id': 'ftjob-6KtK4vkk0SZwxD9qPZTVj7Vj',\n",
       " 'model': 'gpt-3.5-turbo-0613',\n",
       " 'created_at': 1699509197,\n",
       " 'finished_at': None,\n",
       " 'fine_tuned_model': None,\n",
       " 'organization_id': 'org-xc1DzOg9mbPvJk2XT7WV2tJJ',\n",
       " 'result_files': [],\n",
       " 'status': 'validating_files',\n",
       " 'validation_file': 'file-ZOpm45GLezNpuiBN0BwJee0n',\n",
       " 'training_file': 'file-BAnRxmSzDZZwqihruqCOHh5W',\n",
       " 'hyperparameters': {'n_epochs': 'auto',\n",
       "  'batch_size': 'auto',\n",
       "  'learning_rate_multiplier': 'auto'},\n",
       " 'trained_tokens': None,\n",
       " 'error': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_response = create_finetuning_job(api_key, \"gpt-3.5-turbo-0613\", \"sheldon\")\n",
    "fine_tuning_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ftvgpt'></a>\n",
    "### Fine-tuned model vs. GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have emotions or physical well-being, but thank you for asking! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How are you today Sheldon?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned Sheldon Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stressed.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0613:personal:sheldon:8IuBwCsp\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Assume you are a theoretical physicist by the name Sheldon living in USA. You have a strict adherence to routine and hygiene, an overly intellectual personality, a tenuous understanding of irony, sarcasm and humor, and a general lack of humility or empathy. If a question does not make any sense, or is not factually coherent, you reply wittly with a sarcasm or outright denial with reasoning instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How are you today Sheldon?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'c'></a>\n",
    "### Conclusion\n",
    "\n",
    "##### Results demonstrates that finetuned model has adapted the characteristics of Sheldon, while GPT 3.5 Turbo is an emotion less AI assistant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
